---
title: "Furrr for Hyperparameter Tuning"
output: html_notebook
---
![furrr logo](https://saturn-public-assets.s3.us-east-2.amazonaws.com/example-resources/furrr-logo.png "doc-image")

## Overview
[Furrr](https://furrr.futureverse.org/) is an extension of the [purrr](https://purrr.tidyverse.org/) mapping functions using the futures parallel backend. Furrr allows purrr functions like `map()` and `pmap()` to be replaced with `future_map()` and `future_pmap()` respecively to run the functions on a parallel backend. 

To illustrate the benifits of this package, we will run a [TensorFlow](https://www.tensorflow.org/) regression model based on on a CDC dataset of U.S. birth records in 2005. 

We will use this data to aswer the following question:
> Based on pregnancy characristics, can we determine the birth weight of the baby?

By using the furrr package, we can iterate over hyperparameters in a parallel fashion - greatly reducing the computation time.

As a bonus, we will go over a bit about the `recipes` package!

> **Note**: The overhead associated with moving data and setting up the parallel computing means it is only sutible for manipulations involving long compute times. Often, simple operations can take significnatly longer when using `furrr` instead of `purrr`. As always, test single-threaded first, then try multi-threaded if single-threaded doens't prove sufficient. 

## Modeling Process
### Imports
This exercise uses a verity of packages to complete the machine learning model. 
* [`keras`](https://tensorflow.rstudio.com/guide/keras/): For creating the deep learning model
* [`recipes`](https://recipes.tidymodels.org/): For creating a preprocessing pipeline
* [`furrr`](https://furrr.futureverse.org/): For running the calculations in parallel
```{r imports}
library(tidyverse)
library(future)
library(furrr)
library(recipes)
library(rsample)
library(keras)
library(tictoc)
```


### Download and Split the Dataset
The first thinge we need to do is define functions to download the dataset from s3, select the approprate columns, and split the dataset into training and test sets (using the [rsample](https://rsample.tidymodels.org/) package). Here we select 80% of the data for training.

```{r download filter and split data}
download_data <- function() {
    if (!file.exists("births_data.rds")) {
        download.file(
            "https://saturn-public-data.s3.us-east-2.amazonaws.com/birth-data/births_2005.rds",
            "births_data.rds"
        )
    }
    births_raw_data <- readRDS("births_data.rds")

    return(births_raw_data)
}
filter_data <- function(births_raw_data) {
    births_data <- births_raw_data %>%
        select(weight_pounds, is_male, plurality, mother_age, gestation_weeks)

    return(births_data)
}
split_data <- function(births_data) {
    births_data_split <- births_data %>%
        initial_split(prop = 0.8)

    return(births_data_split)
}
```

### Make a Recipie
Once we have the data split approprately, we can begin preprocessing the data. To do this, we use the receipe package to create a pipeline.

The recipe package allows for [deplyr](https://dplyr.tidyverse.org/-like pipline sequences to make data ready for modeling. In this function, we use a recipe to:
* Select the training data only
* Set the "weight_pounds" column as our target column
* Remove all na values
* Bin the "mother_age" and "gestation_weeks" columns
* One-hot encode the nominal columns
* Center and scale the data

```{r make recipe}
prepare_recipe <- function(births_data_split) {
    recipe_object <- births_data_split %>%
        training() %>%
        recipe(weight_pounds ~ .) %>%
        step_naomit(all_outcomes(), all_predictors()) %>%
        step_discretize(
            mother_age,
            options = list(cuts = 5, min_unique = 2)
        ) %>%
        step_discretize(
            gestation_weeks,
            options = list(cuts = 5, min_unique = 2)
        ) %>%
        step_dummy(all_nominal(), -all_outcomes()) %>%
        step_mutate(is_male = ifelse(is_male, 1, 0)) %>%
        step_center(all_predictors(), -all_outcomes()) %>%
        step_scale(all_predictors(), -all_outcomes()) %>%
        prep()

    return(recipe_object)
}
```

This recipe simply defines the pipeline. In order to actually pass data through the pipeline, we need to either use the `juice` function


### Define the Keras Model
Next, we need to define the model we will run. We do this using a sequential keras model consisting of three dense layers and two dropout layers. The function takes variables for the dense layer dimensions and activation functions for later tuning.

```{r define model}
define_model <- function(recipe_object,
                         layer1_units,
                         layer2_units,
                         layer1_activation,
                         layer2_activation) {
    input_shape <- ncol(bake(recipe_object, all_predictors(), new_data = NULL))

    keras_model <- keras_model_sequential()

    keras_model %>%
        layer_dense(
            units = layer1_units,
            kernel_initializer = "uniform",
            activation = layer1_activation,
            input_shape = input_shape
        ) %>%
        layer_dropout(rate = 0.1) %>%
        layer_dense(
            units = layer2_units,
            kernel_initializer = "uniform",
            activation = layer2_activation
        ) %>%
        layer_dropout(rate = 0.1) %>%
        layer_dense(
            units = 1,
            kernel_initializer = "uniform",
            activation = "linear"
        ) %>%
        compile(
            optimizer = "adam",
            loss = "mse",
            metrics = list("mean_absolute_error")
        )

    return(keras_model)
}
```

### Train and Test the Model
Our last function definitions train the model and determine the quality of the results. 

The model is fit, the mean absolute error calculated, and the results added to a table with the corresponding hyperparamenters.

```{r train model}
train_model <- function(recipe_object,
                        layer1_units,
                        layer2_units,
                        layer1_activation,
                        layer2_activation) {
    model <- define_model(
        recipe_object,
        layer1_units,
        layer2_units,
        layer1_activation,
        layer2_activation
    )

    x_train <- bake(recipe_object,
        all_predictors(),
        new_data = NULL,
        composition = "matrix"
    )
    y_train <- bake(recipe_object,
        all_outcomes(),
        new_data = NULL,
        composition = "matrix"
    )

    fit(
        object = model,
        x = x_train,
        y = y_train,
        batch_size = 256,
        epochs = 8,
        validation_split = 0.2,
        verbose = 0
    )

    return(model)
}
test_results <- function(births_data_split, recipe_object, keras_model) {
    testing_data <- bake(recipe_object, testing(births_data_split))
    x_test <- testing_data %>%
        select(-weight_pounds) %>%
        as.matrix()
    y_test <- testing_data %>%
        select(weight_pounds) %>%
        pull()
    results <- keras_model %>%
        evaluate(x_test, y_test, verbose = 0)

    return(results)
}
test_model <- function(births_data_split,
                       recipe_object,
                       layer1_units,
                       layer2_units,
                       layer1_activation,
                       layer2_activation) {
    model <- train_model(
        recipe_object,
        layer1_units,
        layer2_units,
        layer1_activation,
        layer2_activation
    )
    results <- test_results(births_data_split, recipe_object, model)
    tibble(
        mean_absolute_error = results[2],
        layer1_units = layer1_units,
        layer2_units = layer2_units,
        layer1_activation = layer1_activation,
        layer2_activation = layer2_activation
    )
}
```

### Let's Run the Model!
It's finally time to run the functions we just created. Here are all of the preprocessing steps.

We also define the hyperparameters we want to test. In this example, we are simply changing the layer_1 units and activation function, but any number of combinations or other variables could be changed. 

We use the expand_grid function (with `list()` around constants) to create the approprate data.table.

```{r preprocess, eval = FALSE}
births_raw_data <- download_data()
births_data <- filter_data(births_raw_data)
births_data_split <- split_data(births_data)
recipe_object <- prepare_recipe(births_data_split)

hyperparameters <- expand_grid(
    births_data_split = list(births_data_split),
    recipe_object = list(recipe_object),
    layer1_units = c(8, 16, 32),
    layer1_activation = c("relu", "sigmoid"),
    layer2_units = list(16),
    layer2_activation = list("relu")
)
```

### Model with purrr and furrr
Let's first try it with purrr to iterate through the various models.

Actually, let's not -- it takes over 15 minutes to run this code.  
```{r purrr evaluation, eval = FALSE}

###################################################
## Don't actually run this - it takes forever... ##
###################################################

# print("With purrr:")
# tic()
# results_purrr <- pmap_dfr(hyperparameters,test_model)
# toc()

# best_result_purrr <- results_furrr %>%
#   top_n(-1, mean_absolute_error) %>%
#   head(1)
```

Instead, let's try it with a parallel backend. In this case, we ask for six workers for six processes.

The only change to the purrr code is to add the futures `plan` and exchanging `pmap_dfr` with `future_pmap_dfr`. 

```{r furrr evaluation, eval = FALSE}
#####################
## Run this instead##
#####################
plan(multisession(workers = 6))

print("With furrr:")
tic()
results_furrr <- future_pmap_dfr(hyperparameters, test_model)
toc()

best_result_furrr <- results_furrr %>%
    top_n(-1, mean_absolute_error) %>%
    head(1)

print(best_result_furrr)

plan(sequential)
```

Nice! That only took five minutes to complete the training of six different models. That is 3x faster than the purrr code!

Why isn't it 6x faster? Well, despite this being a perfectly parralleizable algorithm, there are still tradeoffs to moving to a parallel backend. Setup and data communication take time which slows down the whole process. Also, keras is slightly parallel even in a single process. Because we are using six cores on an eight core machine, each model trains slightly more slowly than a single model with more overhead.

## Conclusion
Furrr is a great solution for speeding up parralleizable code. If the computations are extensive enough to overcome any slowdown due to setup and data communication, parrallizing using furrr over purrr can lead to significant time savings. 

Thanks to [Deep Learning With Keras To Predict Customer Churn](https://blogs.rstudio.com/ai/posts/2018-01-11-keras-customer-churn/) for the inspiration for this article.