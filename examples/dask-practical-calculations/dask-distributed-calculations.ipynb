{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5fe538d",
   "metadata": {},
   "source": [
    "# Practical Example - Distributed Calculations\n",
    "This example runs hyperparameter tuning on a PyTorch model. It runs multiple models simultaneously using Dask.\n",
    "\n",
    "We will show how to:\n",
    "\n",
    "* Connect to the Dask cluster\n",
    "* Load the data from a shared file system\n",
    "* Run the models\n",
    "* Use the dashboard and logging capabilities of Dask\n",
    "* Implement checkpointing and recovery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60457b38-ee11-4aa3-9d8a-1d73e1c0f2f2",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0187a53-1d8a-4c46-9569-a7ac28f5dd22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T01:04:20.662867Z",
     "iopub.status.busy": "2022-02-25T01:04:20.662589Z",
     "iopub.status.idle": "2022-02-25T01:04:23.018949Z",
     "shell.execute_reply": "2022-02-25T01:04:23.018343Z",
     "shell.execute_reply.started": "2022-02-25T01:04:20.662839Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import traceback\n",
    "\n",
    "import dask\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from dask.distributed import Client\n",
    "from dask_saturn import SaturnCluster\n",
    "from distributed.client import FIRST_COMPLETED, wait\n",
    "from distributed.worker import logger\n",
    "from functions import *\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ababd8-e817-459e-b0c6-93b66ab7b942",
   "metadata": {},
   "source": [
    "## Start the Dask Cluster\n",
    "Here we are starting the Dask cluster. \n",
    "We specify:\n",
    "* The number of workers\n",
    "* Their types\n",
    "* A Client for the cluster\n",
    "\n",
    "We then wait for the workers to start before continuing. This is usually a good practice.\n",
    "\n",
    "The output of this block will be a listing of the client properties including the cluster, scheduler, and workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13cba20-d468-46f1-aa39-98de54e8db95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T01:04:23.022418Z",
     "iopub.status.busy": "2022-02-25T01:04:23.022031Z",
     "iopub.status.idle": "2022-02-25T01:04:27.833040Z",
     "shell.execute_reply": "2022-02-25T01:04:27.832349Z",
     "shell.execute_reply.started": "2022-02-25T01:04:23.022380Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_workers = 3\n",
    "cluster = SaturnCluster(\n",
    "    n_workers=n_workers,\n",
    "    scheduler_size='large',\n",
    "    worker_size='g4dnxlarge'\n",
    ")\n",
    "client = Client(cluster)\n",
    "client.wait_for_workers(n_workers)\n",
    "client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35219ae7-cc55-4434-9793-d7566a9a6e91",
   "metadata": {},
   "source": [
    "## Upload code files to the cluster\n",
    "We need to upload the code file so that the cluster knows about it. Otherwise, the cluster will not be able to import the functions.\n",
    "\n",
    "Because we are just loading one file, we will use the `client.upload_file()` function.\n",
    "\n",
    "We could also use a worker plugin if we required a directory to be uploaded.\n",
    "```python\n",
    "from dask_saturn import RegisterFiles, sync_files\n",
    "\n",
    "client.register_worker_plugin(RegisterFiles())\n",
    "sync_files(client, \"functions\")\n",
    "client.restart()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0646c7d-7440-4b1c-9b2e-b68a42982871",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T01:04:27.834618Z",
     "iopub.status.busy": "2022-02-25T01:04:27.834281Z",
     "iopub.status.idle": "2022-02-25T01:04:27.856702Z",
     "shell.execute_reply": "2022-02-25T01:04:27.855905Z",
     "shell.execute_reply.started": "2022-02-25T01:04:27.834581Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.upload_file(\"functions.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b72056-a6f6-42cf-9866-feea97eb1d43",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define the Delayed Function\n",
    "Input: learning rate\n",
    "Output: results array\n",
    "\n",
    "All of the code should look familiar with the exception of the @dask.delayed at the beginning. This decorator tells Dask to parallelize the function.\n",
    "Outputs are returned and a model is saved every epoch in case of failure.\n",
    "To load the model, simply use the standard PyTorch loading methods:\n",
    "\n",
    "``` python\n",
    "model = TheModelClass(*args, **kwargs)\n",
    "optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a965b5-1019-4e13-b88e-fe9876a74672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T01:04:27.859534Z",
     "iopub.status.busy": "2022-02-25T01:04:27.858037Z",
     "iopub.status.idle": "2022-02-25T01:04:27.874267Z",
     "shell.execute_reply": "2022-02-25T01:04:27.873499Z",
     "shell.execute_reply.started": "2022-02-25T01:04:27.859485Z"
    }
   },
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def train_model(lr):\n",
    "    data_dir = \"/home/jovyan/shared/nathan/poc-gsa/datasets/birds\"\n",
    "    model_dir = \"/home/jovyan/shared/nathan/poc-gsa/models\"\n",
    "    model_version = 1\n",
    "\n",
    "    batch_size = 100\n",
    "    num_epochs = 5\n",
    "\n",
    "    training_start_time = datetime.datetime.now()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    dataloaders, dataset_sizes, classes = load_data(data_dir, batch_size)\n",
    "\n",
    "    model = define_model(len(classes), True)\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", patience=2)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in [\"train\", \"valid\"]:\n",
    "            if phase == \"train\":\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == \"train\":\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == \"train\":\n",
    "                scheduler.step(loss)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            logger.info(\n",
    "                f\"{datetime.datetime.now().isoformat()} - lr {lr} - epoch {epoch} - phase {phase} - loss {epoch_loss} - accuracy {epoch_acc.item()}\"\n",
    "            )\n",
    "\n",
    "        model_path = f\"{model_dir}/model-v{model_version}-lr{lr}-epoch{epoch}-accuracy{round(epoch_acc.item(),2)}\"\n",
    "        new_results = {\n",
    "            \"lr\": lr,\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": epoch_loss,\n",
    "            \"accuracy\": epoch_acc.item(),\n",
    "            \"elapsed_time_sec\": (\n",
    "                datetime.datetime.now() - training_start_time\n",
    "            ).total_seconds(),\n",
    "            \"model_path\": model_path,\n",
    "        }\n",
    "\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"lr\": lr,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": epoch_loss,\n",
    "                \"accuracy\": epoch_acc,\n",
    "            },\n",
    "            model_path,\n",
    "        )\n",
    "\n",
    "        results.append(new_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf999855-1f6e-4e21-b7d4-87961aececd2",
   "metadata": {},
   "source": [
    "## Set up the futures to orchestrate the delayed function\n",
    "\n",
    "To handle errors, we are going to run the delayed function as futures. This allows us to keep track of the function's state and make sure it is computed on the correct resources.\n",
    "\n",
    "First, we map the inputs to the function to get our first set of futures.\n",
    "These futures then need to be gathered from the client so that we can compute them.\n",
    "We finally compute the futures on the worker processes.\n",
    "\n",
    "Note that we specify the resources used per worker to limit the number of tasks that can be run simultaneously per worker. If this was not specified, Dask would allocate multiple tasks at a time to the workers, resulting in CUDA memory errors on the GPUs. By specifying that the task requires one GPU, Dask knows to send only one task per worker. We could alternatively reduce the batch size to accommodate the memory requirements of the multiple calculations.\n",
    "\n",
    ">**Note**: The functions will run on the Dask cluster even if the client kernel dies, unless you specify otherwise (`shutdown_on_close` option in cluster setup). It is important that you run `client.restart()` to clear the Dask task graph if you want to discontinue calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77405183-5ad1-4bb5-9b8a-3701a379732b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T01:04:27.875948Z",
     "iopub.status.busy": "2022-02-25T01:04:27.875570Z",
     "iopub.status.idle": "2022-02-25T01:04:27.922577Z",
     "shell.execute_reply": "2022-02-25T01:04:27.921714Z",
     "shell.execute_reply.started": "2022-02-25T01:04:27.875910Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rates = np.arange(0.0005, 0.0035, 0.0005)\n",
    "train_future = client.map(train_model, learning_rates)\n",
    "futures_gathered = client.gather(train_future)\n",
    "futures_computed = client.compute(futures_gathered, resources={\"gpu\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d3ff18-ae12-4af6-b017-e3d27afca5e7",
   "metadata": {},
   "source": [
    "## Handle Errors\n",
    "Dask is typically resilient to errors in the task graph (for instance losing a worker), but sometimes errors can occur in your code. To handle these, we create a queue of futures that will output the results or the appropriate error if required. \n",
    "\n",
    "Since this function is fairly complicated at first glance, let’s break it down.\n",
    "\n",
    "``` python\n",
    "queue = c.compute(results)\n",
    "futures_to_index = {fut: i for i, fut in enumerate(queue)}\n",
    "results = [None for x in range(len(queue))]\n",
    "```\n",
    "\n",
    "We call compute on results, but since we’re not passing sync=True, we immediately get back futures, which represent the computation, which has not completed yet. We also create a mapping from the future itself, to the _n_th input argument that generated it. Finally, we populate a list of results filled with Nones for now.\n",
    "```python\n",
    "while queue:\n",
    "    result = wait(queue, return_when=FIRST_COMPLETED)\n",
    "```\n",
    "\n",
    "Next, we wait for results, and we process them as they come in. When we wait for futures, they are separated into futures that are done, and those that are not_done.\n",
    "```python\n",
    "        if future.status == 'finished':\n",
    "            print(f'finished computation #{index}')\n",
    "            results[index] = future.result()\n",
    "```\n",
    "If the future is finished, then we print that we succeeded, and we store the result.\n",
    "```python\n",
    "        else:\n",
    "            print(f'errored #{index}')\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                results[index] = e\n",
    "                traceback.print_exc()\n",
    "```\n",
    "Otherwise, we store the exception and print the stack trace.\n",
    "```python\n",
    "    queue = result.not_done\n",
    "```\n",
    "Finally, we set the queue to those futures that have not yet been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f8cec-b724-48df-8dc7-aa5077b024e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T01:04:27.924349Z",
     "iopub.status.busy": "2022-02-25T01:04:27.923920Z",
     "iopub.status.idle": "2022-02-25T01:21:06.765373Z",
     "shell.execute_reply": "2022-02-25T01:21:06.764587Z",
     "shell.execute_reply.started": "2022-02-25T01:04:27.924311Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "queue = client.compute(futures_computed)\n",
    "futures_to_index = {fut: i for i, fut in enumerate(queue)}\n",
    "results = [None for x in range(len(queue))]\n",
    "\n",
    "while queue:\n",
    "    result = wait(queue, return_when=FIRST_COMPLETED)\n",
    "    for future in result.done:\n",
    "        index = futures_to_index[future]\n",
    "        if future.status == \"finished\":\n",
    "            print(f\"Finished computation #{index}\")\n",
    "            results[index] = future.result()\n",
    "        else:\n",
    "            print(f\"Error #{index}\")\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                results[index] = e\n",
    "                traceback.print_exc()\n",
    "    queue = result.not_done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c6fbe",
   "metadata": {},
   "source": [
    "## Shut Down the Cluster\n",
    "At this point, all cluster calculations are complete, so we can shut down the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbd747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fe2e0b-6bcd-44df-b66d-1216bab61a93",
   "metadata": {},
   "source": [
    "## View the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ec06a9-cf16-4e92-9e51-796b3a4548f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T01:21:06.767048Z",
     "iopub.status.busy": "2022-02-25T01:21:06.766694Z",
     "iopub.status.idle": "2022-02-25T01:21:09.736694Z",
     "shell.execute_reply": "2022-02-25T01:21:09.736031Z",
     "shell.execute_reply.started": "2022-02-25T01:21:06.767011Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_concatenated = [item for sublist in results for item in sublist]\n",
    "results_df = pd.DataFrame.from_dict(results_concatenated)\n",
    "results_df[\"lr\"] = results_df[\"lr\"].astype(str)\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "sns.relplot(data=results_df, x=\"epoch\", y=\"loss\", col=\"lr\", kind=\"line\")\n",
    "\n",
    "sns.relplot(data=results_df, x=\"elapsed_time_sec\", y=\"loss\", col=\"lr\", kind=\"line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0acd04-a4ab-4281-ac48-185d38e3faa7",
   "metadata": {},
   "source": [
    "## Load the Best Model\n",
    "Finally, we want to load the best model and see what our test scores are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394c93e-b15a-434f-b2fb-737c569218e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T01:21:09.737893Z",
     "iopub.status.busy": "2022-02-25T01:21:09.737653Z",
     "iopub.status.idle": "2022-02-25T01:21:09.743316Z",
     "shell.execute_reply": "2022-02-25T01:21:09.742675Z",
     "shell.execute_reply.started": "2022-02-25T01:21:09.737869Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df.loc[results_df[\"loss\"].idxmin(), \"model_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42774d1e-114a-4502-886b-4033cf80c81a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T01:23:36.263550Z",
     "iopub.status.busy": "2022-02-25T01:23:36.263228Z",
     "iopub.status.idle": "2022-02-25T01:23:37.160987Z",
     "shell.execute_reply": "2022-02-25T01:23:37.160336Z",
     "shell.execute_reply.started": "2022-02-25T01:23:36.263525Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloaders, dataset_sizes, classes = load_data(\n",
    "    \"C:/Users/narob/Documents/Code/SaturnCloud/data/birds\", 100\n",
    ")\n",
    "model_path = \"C:/Users/narob/Documents/Code/SaturnCloud/data/models/model-v1-lr0.003-epoch4-accuracy1.0\"\n",
    "\n",
    "# For use on hosted notebook\n",
    "#\n",
    "# dataloaders, dataset_sizes, classes = load_data(\n",
    "#     \"/home/jovyan/shared/nathan/poc-gsa-folder/datasets/birds\", 100\n",
    "# )\n",
    "# model_path = results_df.loc[results_df[\"loss\"].idxmin(), \"model_path\"]\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = define_model(len(classes), False)\n",
    "model = model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "\n",
    "checkpoint = torch.load(model_path, map_location=torch.device(device))\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "epoch = checkpoint[\"epoch\"]\n",
    "loss = checkpoint[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa52e1c-6054-4a0e-a757-f245807e99ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T01:23:37.649344Z",
     "iopub.status.busy": "2022-02-25T01:23:37.649044Z",
     "iopub.status.idle": "2022-02-25T01:23:37.654698Z",
     "shell.execute_reply": "2022-02-25T01:23:37.653963Z",
     "shell.execute_reply.started": "2022-02-25T01:23:37.649320Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, iterator):\n",
    "    model.eval()\n",
    "    labels = []\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in iterator:\n",
    "            x = x.to(device)\n",
    "            y_pred = model(x)\n",
    "            y_prob = nn.functional.softmax(y_pred, dim=-1)\n",
    "            labels.append(y.cpu())\n",
    "            probs.append(y_prob.cpu())\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    probs = torch.cat(probs, dim=0)\n",
    "    return labels, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e2636-3391-461f-97eb-77e085406359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T01:23:38.047178Z",
     "iopub.status.busy": "2022-02-25T01:23:38.046868Z",
     "iopub.status.idle": "2022-02-25T01:23:39.829032Z",
     "shell.execute_reply": "2022-02-25T01:23:39.828282Z",
     "shell.execute_reply.started": "2022-02-25T01:23:38.047154Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels, probs = get_predictions(model, dataloaders[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb27ae91-7f96-4b8d-a925-bc1301008536",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T01:23:39.830955Z",
     "iopub.status.busy": "2022-02-25T01:23:39.830622Z",
     "iopub.status.idle": "2022-02-25T01:23:39.835140Z",
     "shell.execute_reply": "2022-02-25T01:23:39.834383Z",
     "shell.execute_reply.started": "2022-02-25T01:23:39.830927Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_labels = torch.argmax(probs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605911b8-2746-4d42-a740-ffd58486f464",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T01:23:39.836839Z",
     "iopub.status.busy": "2022-02-25T01:23:39.836460Z",
     "iopub.status.idle": "2022-02-25T01:23:39.843746Z",
     "shell.execute_reply": "2022-02-25T01:23:39.843088Z",
     "shell.execute_reply.started": "2022-02-25T01:23:39.836803Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(labels, pred_labels, classes):\n",
    "    fig = plt.figure(figsize=(50, 50))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    cm = confusion_matrix(labels, pred_labels)\n",
    "    cm = ConfusionMatrixDisplay(cm, display_labels=classes)\n",
    "    cm.plot(values_format=\"d\", cmap=\"Blues\", ax=ax)\n",
    "    fig.delaxes(fig.axes[1])  # delete colorbar\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=50)\n",
    "    plt.ylabel(\"True Label\", fontsize=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b467d9b-a06b-4462-aa5c-b31c8f6858bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T01:23:39.845901Z",
     "iopub.status.busy": "2022-02-25T01:23:39.845552Z",
     "iopub.status.idle": "2022-02-25T01:23:51.409911Z",
     "shell.execute_reply": "2022-02-25T01:23:51.408933Z",
     "shell.execute_reply.started": "2022-02-25T01:23:39.845870Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(labels, pred_labels, classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
