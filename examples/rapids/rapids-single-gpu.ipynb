{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use RAPIDS on a Single GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RAPIDS Logo](https://saturn-public-assets.s3.us-east-2.amazonaws.com/example-resources/rapids.png)\n",
    "\n",
    "RAPIDS is a collection of libraries that enable you to take advantage of NVIDIA GPUs to accelerate machine learning workflows. Minimal changes are required to transition from familiar `pandas` and `scikit-learn` code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This example describes how to run a machine learning training workflow using the famous NYC Taxi Dataset. This dataset contains information about taxi trips in New York City. For the purposes of this example, we will be looking at the Yellow Taxi data from January 2019.\n",
    "We will use this data to answer the following question:\n",
    "> Based on characteristics that can be known at the beginning of a trip, will this trip result in a good tip?\n",
    "\n",
    "This exercise uses the following RAPIDS packages to execute code on a GPU, rather than a CPU:\n",
    "\n",
    "* [`cudf`](https://github.com/rapidsai/cudf): data frame manipulation, similar to `pandas`\n",
    "* [`cuml`](https://github.com/rapidsai/cuml): machine learning training and evaluation, similar to `scikit-learn`\n",
    "\n",
    "For more information on RAPIDS, see [\"Getting Started\"](https://rapids.ai/start.html) in the RAPIDS docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cudf\n",
    "import datetime as dt\n",
    "from cuml.ensemble import RandomForestClassifier\n",
    "from cuml.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Your Saturn Cloud Environment\n",
    "The easiest way to get started with this example is to use the RAPIDS template resource. Log on to your Saturn Cloud account and click on the RAPIDS template on the main screen. \n",
    "\n",
    "If you want to create a custom resource, use the `saturncloud/saturn-rapids` image to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Dataset\n",
    "\n",
    "The first thing we want to do is load in the NYC Taxi Trip dataset. The code below loads the data into a `cudf` data frame. This is similar to a `pandas` dataframe, but it lives in GPU memory and most operations on it are done on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = cudf.read_csv(\n",
    "    \"https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2019-01.csv\",\n",
    "    parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`parse_dates` tells `cudf` to interpret the \"tpep_pickup_datetime\" and \"tpep_dropoff_datetime\" columns in the datasets as datetime columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Analysis on the Raw Data\n",
    "\n",
    "Many dataframe operations that you would execute on a `pandas` dataframe also work for a `cudf` dataframe.\n",
    "\n",
    "You can view the first 10 rows of the data using the `.head()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the datatypes of the data using the `.dtypes` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can compute the length and memory usage of the dataset using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = len(taxi)\n",
    "memory_usage = round(taxi.memory_usage(deep=True).sum() / 1e9, 2)\n",
    "print(f\"Num rows: {num_rows}, Memory Usage: {memory_usage} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Data\n",
    "The raw data we downloaded needs to be processed before we can use it to train our machine learning model. We need to do things like create a target column, add additional features, and remove unnecessary columns. We will wrap everything in a function so we can use it later when we need to prepare data for testing or implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_df(df: cudf.DataFrame) -> cudf.DataFrame:\n",
    "    \n",
    "    df = df[df['fare_amount'] > 0] #to avoid a divide by zero error\n",
    "    df['tip_fraction'] = df['tip_amount'] / df['fare_amount']\n",
    "    df['target'] = df['tip_fraction'] > 0.2\n",
    "\n",
    "    df['pickup_weekday'] = df['tpep_pickup_datetime'].dt.weekday\n",
    "    df['pickup_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
    "    df['pickup_week_hour'] = (df['pickup_weekday'] * 24) + df.pickup_hour\n",
    "    df['pickup_minute'] = df['tpep_pickup_datetime'].dt.minute\n",
    "    \n",
    "    df = df[['pickup_weekday',\n",
    "             'pickup_hour',\n",
    "             'pickup_week_hour',\n",
    "             'pickup_minute',\n",
    "             'passenger_count',\n",
    "             'PULocationID',\n",
    "             'DOLocationID',\n",
    "             'target'\n",
    "            ]]\n",
    "    \n",
    "    df = df.astype('float32').fillna(-1)\n",
    "    df['target'] = df['target'].astype('int32')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function does the following:\n",
    "\n",
    "|||\n",
    "|---|---|\n",
    "|**Creates a target column**|The raw data don't contain a column that cleanly describes whether a particular ride could be considered a \"good\" tip. So we need to add one! In this exercise, a \"good\" tip is one that is greater than 20% of the fare amount. This function makes this a binary classification problem.|\n",
    "|**Adds additional features**|Here, instead of giving a model a raw timestamp, we provide multiple derived characteristics like hour of the day and day of the week. Giving the machine learning model a richer description of each training observation improves its ability to describe the relationship between those observations' characteristics and the target. |\n",
    "|**Removes Unused Columns**|If the goal is to produce a model that could predict whether a tip will be good *at the beginning of the trip*, then characteristics that can only be known **AFTER** the tip must be excluded. For example, you can't know the dropoff time or the type of payment until a ride has concluded.|\n",
    "|**Cast the Columns to 32-bit types**|This reduces the memory footprint of the model and insures compatablility with older version of `cuml`.\n",
    "\n",
    "We now just need to run the raw data through our function to prepare it for machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = prep_df(taxi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Analysis on the Preprocessed Data\n",
    "\n",
    "Since this is a binary classification task, before proceeding we should examine the proportion of 1s and 0s in the target. This can be done with the `value_counts()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi['target'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also check the first few rows of the dataset using the `.head()` function to make sure that the features look reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the dataframe has been processed, let’s check its length and size in memory again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = len(taxi)\n",
    "memory_usage = round(taxi.memory_usage(deep=True).sum() / 1e9, 2)\n",
    "print(f\"Num rows: {num_rows}, Memory Usage: {memory_usage} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing unused columns dropped the size of the training data to about one-third the size of the raw data. You can also see that the dataset lost a few rows with zero fare amounts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Random Forest Model\n",
    "\n",
    "Now that the data has been prepped, it’s time to build a model!\n",
    "\n",
    "For this task, we'll use the `RandomForestClassifier` from `cuml`. If you've never used a random forest or need a refresher, consult [\"Forests of randomized trees\"](https://scikit-learn.org/stable/modules/ensemble.html#forest) in the `scikit-learn` documentation.\n",
    "\n",
    "First, we define the X and y variables for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = taxi.drop(columns = ['target'])\n",
    "y = taxi['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the model with the following parameters:\n",
    "- `n_estimators=100` = create a 100-tree forest\n",
    "- `max_depth=10` = stop growing a tree once it contains a leaf node that is 10 levels below the root\n",
    "- `n_streams` - create four decision trees at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=10, n_streams=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing any of these parameters will change the training time, memory requirements, and model accuracy. Feel free to play around with these parameters!\n",
    "\n",
    "And, finally, we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "_ = rfc.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should take about 30 seconds to run. The `%%time` command before the function prints out exactly how long it takes to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Metrics on a Test Set \n",
    "\n",
    "We will use another month of taxi data for the test set and calculate the AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_test = cudf.read_csv(\n",
    "    \"https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2019-02.csv\",\n",
    "    parse_dates=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating predictions on this new dataset, it has to be transformed in exactly the way that the original training data were prepared. Thankfully you have already wrapped that transformation logic in a function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_test = prep_df(taxi_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cuml` comes with many functions for calculating metrics that describe how well a model's predictions match the actual values. This tutorial uses the `roc_auc_score()` to evaluate the model. This metric measures the area under the receiver operating characteristic curve. Values closer to 1.0 are desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = taxi_test.drop(columns = ['target'])\n",
    "y_test = taxi_test['target']\n",
    "\n",
    "preds = rfc.predict_proba(X_test)[1]\n",
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph the ROC Curve\n",
    "\n",
    "Finally, let’s look at the ROC curve. `cuml` does not have a ROC curve function, so we convert the target column and predictions to `numpy` arrays and use the `sklearn` `roc_curve` function. Plotting is done by the `seaborne` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test.to_array(), preds.to_array())\n",
    "\n",
    "sns.set(font_scale = 2)\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.plot(fpr,tpr , color='red')\n",
    "plt.legend(['Random chance','ROC curve'])\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.xlim([0,1])\n",
    "plt.ylim([0,1])\n",
    "plt.fill_between(fpr,tpr, color='yellow', alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows that our results were only moderately better than random chance. It is possible that tuning hyperparameters and giving the model additional features and training data will improve this outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this example, we showed how to use the RAPIDS library with a single GPU to train a model for a binary classification task. We [downloaded data from s3](ref), preprocessed the data, ran the model, tested the model, and calculated and plotted the accuracy.\n",
    "\n",
    "See our other RAPIDS tutorial for how you can do the same problem [using a GPU cluster](./rapids-gpu-cluster.ipynb)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
