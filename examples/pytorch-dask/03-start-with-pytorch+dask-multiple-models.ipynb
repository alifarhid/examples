{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pet names distributed experiment\n",
    "\n",
    "This notebook takes the same neural network training job as the [pet-names-single.ipynb](pet-names-single.ipynb) file, but instead of training the model once it trains it several times with different parameters and compares the results. Rather than having to wait for each of the different set of parameters to train sequentially, it uses the dask cluster to train them all concurrently, dramatically speeding up the process.\n",
    "\n",
    "_all the set up code is the exact same as the original notebook:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import uuid\n",
    "import datetime\n",
    "import pickle\n",
    "import json\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"seattle_pet_licenses_cleaned.json\") as f:\n",
    "    pet_names = json.loads(f.read())\n",
    "\n",
    "# Our list of characters, where * represents blank and + represents stop\n",
    "characters = list(\"*+abcdefghijklmnopqrstuvwxyz-. \")\n",
    "str_len = 8\n",
    "\n",
    "def format_training_data(pet_names, device=None):\n",
    "    def get_substrings(in_str):\n",
    "        # add the stop character to the end of the name, then generate all the partial names\n",
    "        in_str = in_str + \"+\"\n",
    "        res = [in_str[0: j] for j in range(1, len(in_str) + 1)]\n",
    "        return res\n",
    "    pet_names_expanded = [get_substrings(name) for name in  pet_names]\n",
    "    pet_names_expanded = [item for sublist in pet_names_expanded for item in sublist]\n",
    "    pet_names_characters = [list(name) for name in pet_names_expanded]\n",
    "    pet_names_padded = [name[-(str_len + 1):] for name in pet_names_characters]\n",
    "    pet_names_padded = [list((str_len + 1- len(characters)) * \"*\") + characters for characters in pet_names_padded]\n",
    "    pet_names_numeric = [[characters.index(char) for char in name] for name in pet_names_padded]\n",
    "\n",
    "    # the final x and y data to use for training the model. Note that the x data needs to be one-hot encoded\n",
    "    if device is None:\n",
    "        y = torch.tensor([name[1:] for name in pet_names_numeric])\n",
    "        x = torch.tensor([name[:-1] for name in pet_names_numeric])\n",
    "    else:\n",
    "        y = torch.tensor([name[1:] for name in pet_names_numeric], device = device)\n",
    "        x = torch.tensor([name[:-1] for name in pet_names_numeric], device = device)\n",
    "    x = torch.nn.functional.one_hot(x, num_classes = len(characters)).float()\n",
    "    return x, y\n",
    "\n",
    "class OurDataset(Dataset):\n",
    "    def __init__(self, pet_names, device=None):\n",
    "        self.x, self.y = format_training_data(pet_names, device)\n",
    "        self.permute()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        idx = self.permutation[idx]\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def permute(self):\n",
    "        self.permutation = torch.randperm(len(self.x))\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm_size = 128\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=len(characters),\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=4,\n",
    "            batch_first=True,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, len(characters))\n",
    "    def forward(self, x):\n",
    "        output, state = self.lstm(x)\n",
    "        logits = self.fc(output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to run the model multiple times in parallel\n",
    "\n",
    "Below is the code to run the model multiple times concurrently in a distributed way. First, we set up the dask cluster and appropriate libraries.\n",
    "\n",
    "**_(if this code does not correctly run you may want to restart the dask cluster from the project page)_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dask-saturn:Cluster is ready\n",
      "INFO:dask-saturn:Registering default plugins\n",
      "INFO:dask-saturn:{'tcp://10.0.0.240:37427': {'status': 'repeat'}, 'tcp://10.0.11.208:44927': {'status': 'repeat'}, 'tcp://10.0.21.187:41303': {'status': 'repeat'}, 'tcp://10.0.7.34:40335': {'status': 'repeat'}}\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import dask\n",
    "from dask_saturn import SaturnCluster\n",
    "from dask.distributed import Client\n",
    "from distributed.worker import logger\n",
    "\n",
    "cluster = SaturnCluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to run the model multiple times in parallel starts with a training function that is extremely similar to the original one. The changes are:\n",
    "\n",
    "* The function has a @dask.delayed indicator at the top so dask knows to parallelize it\n",
    "* The training function now has an input parameter `experiment` which is a tuple containing the batch size and learning rate\n",
    "* Instead of printing the results, we now pass them to `logger.info()` so that they show up in the [dask logs](man/logging.md)\n",
    "* Instead of saving the actual model, we create an array of results that the model returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def train(experiment):\n",
    "    num_epochs = 8\n",
    "    batch_size, lr = experiment\n",
    "    training_start_time = datetime.datetime.now()\n",
    "    device = torch.device(0)\n",
    "\n",
    "    dataset = OurDataset(pet_names, device = device)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    model = Model()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        dataset.permute()\n",
    "        for i, (batch_x, batch_y) in enumerate(loader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_y_pred = model(batch_x)\n",
    "            \n",
    "            loss = criterion(batch_y_pred.transpose(1, 2), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            logger.info(f\"{datetime.datetime.now().isoformat()} - batch {i} - batch_size {batch_size} - lr {lr} - epoch {epoch} complete - loss {loss.item()}\")\n",
    "        epoch_end_time = datetime.datetime.now().isoformat()\n",
    "        new_results = {\n",
    "            \"batch_size\": batch_size,\n",
    "            \"lr\": lr,\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": loss.item(),\n",
    "            \"elapsed_time_sec\": (datetime.datetime.now() - training_start_time).total_seconds()\n",
    "        }\n",
    "        results.append(new_results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the parallel code\n",
    "\n",
    "The code is executed across multiple machines by running the cell below. It takes the list of (batch size, learning rate) tuples and passes them to the dask map function, that get combined into a single result with gather and compute. Since dask is lazy, the computation doesn't actually begin until you run the last line of the cell and request the results. You won't see any messages show up as the model is being trained since all of the output is captured i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [(4096,0.001), (16384, 0.001), (4096,0.01), (16384, 0.01)]\n",
    "\n",
    "train_future = client.map(train, inputs)\n",
    "futures_gathered = client.gather(train_future)\n",
    "futures_computed = client.compute(futures_gathered)\n",
    "results = [x.result() for x in futures_computed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the results\n",
    "\n",
    "Here is where we show the results of the experiment. As you can see, the smallest batch size with highest learning rate quickly convered on a solution with fewer epochs than the other sets of parameters. They all needed a similar amount of clock time although the high batch size high learning rate needed slightly longer to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_concatenated = [item for sublist in results for item in sublist]\n",
    "results_df = pd.DataFrame.from_dict(results_concatenated)\n",
    "results_df['experiment'] = \"bs=\" + results_df['batch_size'].astype(str) + \" lr=\" + results_df['lr'].astype(str)\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "sns.relplot(\n",
    "    data=results_df,\n",
    "    x=\"epoch\", y=\"loss\", col=\"experiment\", kind=\"line\"\n",
    ")\n",
    "\n",
    "sns.relplot(\n",
    "    data=results_df,\n",
    "    x=\"elapsed_time_sec\", y=\"loss\", col=\"experiment\", kind=\"line\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
