{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19924702",
   "metadata": {},
   "source": [
    "# Practical Example - Distributed Data\n",
    "\n",
    "This example runs a simple backtest on a large dataset (~29GB compressed) using Dask.\n",
    "\n",
    "We will distribute the dataset over the Dask cluster, run the analysis, and get the results.\n",
    "\n",
    "We will show how to do the following:\n",
    "\n",
    "* Connect to the Dask cluster.\n",
    "* Load the data from a shared file system.\n",
    "* Use Dask dataframes to store the data.\n",
    "* Run the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f28c5-915b-4375-92e0-e367e679b0e1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e961f4-b8a4-4c33-8827-8a7ea4dedf9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T19:48:27.526401Z",
     "iopub.status.busy": "2022-02-25T19:48:27.526076Z",
     "iopub.status.idle": "2022-02-25T19:48:28.782883Z",
     "shell.execute_reply": "2022-02-25T19:48:28.782293Z",
     "shell.execute_reply.started": "2022-02-25T19:48:27.526333Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.distributed import Client, wait\n",
    "from dask_saturn import SaturnCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed6d994-98fa-44e2-82bb-23075898c6a8",
   "metadata": {},
   "source": [
    "## Starting the Dask Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b077f6-7026-440e-9719-dee51dd55715",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T19:48:28.784576Z",
     "iopub.status.busy": "2022-02-25T19:48:28.784343Z",
     "iopub.status.idle": "2022-02-25T19:48:30.419773Z",
     "shell.execute_reply": "2022-02-25T19:48:30.419160Z",
     "shell.execute_reply.started": "2022-02-25T19:48:28.784544Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_workers = 5\n",
    "cluster = SaturnCluster(n_workers=n_workers)\n",
    "client = Client(cluster)\n",
    "client.wait_for_workers(n_workers=n_workers)\n",
    "client.restart()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2241e0d3-e371-46a5-8345-8d646f0fc738",
   "metadata": {},
   "source": [
    "## Read in the File\n",
    "This file is approximately 28GB compressed. It is stored in parquet format, but could be any supported file type (e.g., csv, json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a4322-ecfd-4c74-ab88-1cae72d280cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T19:49:06.607781Z",
     "iopub.status.busy": "2022-02-25T19:49:06.607486Z",
     "iopub.status.idle": "2022-02-25T19:49:06.733485Z",
     "shell.execute_reply": "2022-02-25T19:49:06.732731Z",
     "shell.execute_reply.started": "2022-02-25T19:49:06.607756Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf = dd.read_parquet(\n",
    "    \"/home/jovyan/shared/nathan/poc-gsa/datasets/stocks/stock_data.pq\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92f10a7-9ec4-4f99-a474-b27717c7ab6f",
   "metadata": {},
   "source": [
    "## Persist the file\n",
    "This is not strictly necessary, but can be useful if you are doing analysis on the file. This method saves the data to the Dask workers' memory. If you do not persist, any time you call `.compute()` in this sequence, you will re-load the file.\n",
    "\n",
    "`wait()` halts the progress until the persistance is done. This avoids some weird errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc9b70a-7d03-4c16-93e6-7916e10de776",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T19:49:12.057225Z",
     "iopub.status.busy": "2022-02-25T19:49:12.056936Z",
     "iopub.status.idle": "2022-02-25T19:51:02.005948Z",
     "shell.execute_reply": "2022-02-25T19:51:02.005247Z",
     "shell.execute_reply.started": "2022-02-25T19:49:12.057200Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf = ddf.persist()\n",
    "_ = wait(ddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6e393a-d2a2-4181-ac75-1e0f153e0af3",
   "metadata": {},
   "source": [
    "## Conduct the Backtest\n",
    "This is a simple moving average crossover strategy. Nothing special here; this should look very similar to pandas code.\n",
    "\n",
    "The main difference is the introduction of `meta=(column, type)`. Because Dask is lazy and doesn't look at the whole column until `.compute()` is called, it can sometimes get column types wrong. Specifying the column types directly is usually a good idea for this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b0a6ef-97d4-41d7-89d6-179c9b078f64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T19:51:02.007787Z",
     "iopub.status.busy": "2022-02-25T19:51:02.007543Z",
     "iopub.status.idle": "2022-02-25T19:51:02.061927Z",
     "shell.execute_reply": "2022-02-25T19:51:02.061202Z",
     "shell.execute_reply.started": "2022-02-25T19:51:02.007756Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf[\"signal\"] = (\n",
    "    ddf[\"ask_close\"].rolling(5 * 60).mean() - ddf[\"ask_close\"].rolling(20 * 60).mean()\n",
    ")\n",
    "\n",
    "ddf[\"position\"] = (ddf[\"signal\"].apply(np.sign, meta=(\"ask_close\", \"float64\")) + 1) / 2\n",
    "\n",
    "ddf[\"return\"] = ddf[\"position\"].shift(1) * ddf[\"ask_close\"].apply(\n",
    "    np.log, meta=(\"ask_close\", \"float64\")\n",
    ").diff(1)\n",
    "\n",
    "ddf[\"total\"] = ddf[\"return\"].cumsum().apply(np.exp, meta=(\"return\", \"float64\"))\n",
    "\n",
    "ddf_results = ddf[[\"total\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8198f38-8362-4e7a-9f58-c57687c31f20",
   "metadata": {},
   "source": [
    "## Compute\n",
    "Lastly, we compute the Dask dataframe using the `.tail()` command. This takes the data from Dask worker memory and brings it back to the client machine. We are only returning one value, so there should be no memory issues here, but be cognizant of how much data you are bringing back to your local machine so as not to crash the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54217095-95fe-45b4-800a-f95bbc60a9be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-25T19:51:02.065392Z",
     "iopub.status.busy": "2022-02-25T19:51:02.065209Z",
     "iopub.status.idle": "2022-02-25T19:51:08.016308Z",
     "shell.execute_reply": "2022-02-25T19:51:08.015745Z",
     "shell.execute_reply.started": "2022-02-25T19:51:02.065371Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_returns = ddf[\"total\"].tail(1)\n",
    "\n",
    "print(total_returns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
